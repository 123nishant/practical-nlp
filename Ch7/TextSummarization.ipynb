{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "TextSummarization.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDRrAaF6B0Qd",
        "colab_type": "text"
      },
      "source": [
        "## Text Summarization\n",
        "\n",
        "### There are broadly two types of summarization — Extractive and Abstractive\n",
        "\n",
        "    1. Extractive— These approaches select sentences from the corpus that best represent it and arrange them to form a summary.\n",
        "    2. Abstractive— These approaches use natural language techniques to summarize a text using novel sentences.\n",
        "\n",
        "In this notebook, let us see a few examples of existing summarization approaches.\n",
        "The first one comes from the python library sumy, which implements several popular summarization approaches from literature. The second example uses gensim's summarizer implementation. Then we move on to Summa and finally we wrap up extractive summarization using BERT. \n",
        "\n",
        "\n",
        "#### [TODO give an intro to abstractive vs extractive summarization here?]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlIo08bpB0Qm",
        "colab_type": "text"
      },
      "source": [
        "## Summarization with Sumy\n",
        "\n",
        "### Sumy offers several algorithms and methods for summarization such as:\n",
        "\n",
        "\n",
        "#### [Not sure if we should give a brief information of the different algorithms mentioned below or just list them]\n",
        "    1. Luhn – Heurestic method\n",
        "    2. Latent Semantic Analysis\n",
        "    4. LexRank – Unsupervised approach inspired by algorithms PageRank and HITS\n",
        "    5. TextRank - Graph-based summarization technique with keyword extractions in from document\n",
        "There are many more which you can find in the github repo of [sumy](https://github.com/miso-belica/sumy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fE-e9HKKB0Qv",
        "colab_type": "code",
        "colab": {},
        "outputId": "fcecd5af-f0d5-4245-b3f3-849815c26fb3"
      },
      "source": [
        "!pip install sumy #install sumy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sumy in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (0.8.1)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from sumy) (19.8.18)\n",
            "Requirement already satisfied: requests>=2.7.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from sumy) (2.23.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from sumy) (3.5)\n",
            "Requirement already satisfied: lxml>=2.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from breadability>=0.1.20->sumy) (4.5.0)\n",
            "Requirement already satisfied: chardet in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from breadability>=0.1.20->sumy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from requests>=2.7.0->sumy) (1.25.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from requests>=2.7.0->sumy) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from requests>=2.7.0->sumy) (2018.8.24)\n",
            "Requirement already satisfied: tqdm in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from nltk>=3.0.2->sumy) (4.46.0)\n",
            "Requirement already satisfied: click in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from nltk>=3.0.2->sumy) (7.1.2)\n",
            "Requirement already satisfied: regex in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from nltk>=3.0.2->sumy) (2020.4.4)\n",
            "Requirement already satisfied: joblib in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from nltk>=3.0.2->sumy) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVKqAHCEB0R0",
        "colab_type": "code",
        "colab": {},
        "outputId": "8723b0bf-d3a8-4d08-de38-61c0d242647b"
      },
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/etherealenvy/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVu_YDlXB0SR",
        "colab_type": "code",
        "colab": {},
        "outputId": "396bccc7-eeeb-43c4-dc60-a4c718d2dd67"
      },
      "source": [
        "#Code to summarize a given webpage using Sumy's TextRank implementation. \n",
        "from sumy.parsers.html import HtmlParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "num_sentences_in_summary = 2\n",
        "url = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\n",
        "parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n",
        "\n",
        "summarizer_list=(\"TextRankSummarizer:\",\"LexRankSummarizer:\",\"LuhnSummarizer:\",\"LsaSummarizer\") #list of summarizers\n",
        "summarizers = [TextRankSummarizer(), LexRankSummarizer(), LuhnSummarizer(), LsaSummarizer()]\n",
        "\n",
        "for i,summarizer in enumerate(summarizers):\n",
        "    print(summarizer_list[i])\n",
        "    for sentence in summarizer(parser.document, num_sentences_in_summary):\n",
        "        print((sentence))\n",
        "    print(\"-\"*30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextRankSummarizer:\n",
            "For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.\n",
            "Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n",
            "------------------------------\n",
            "LexRankSummarizer:\n",
            "An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.\n",
            "The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n",
            "------------------------------\n",
            "LuhnSummarizer:\n",
            "This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).\n",
            "A Class of Submodular Functions for Document Summarization \", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011 ^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization , In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.\n",
            "------------------------------\n",
            "LsaSummarizer\n",
            "For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.\n",
            "Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney’s seminal paper.\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5Qyd9M2B0Ss",
        "colab_type": "text"
      },
      "source": [
        "Clearly there are other summarizers and options in sumy. We leave their exploration as an exercise to you!\n",
        "\n",
        "## Summarization example with Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZOIdGesB0Sy",
        "colab_type": "code",
        "colab": {},
        "outputId": "373be4d4-565b-46a2-be01-d3ccbb8e3fc5"
      },
      "source": [
        "!pip install gensim #installation of the library"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 117 kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from gensim) (1.18.4)\n",
            "Collecting smart-open>=1.8.1\n",
            "  Using cached smart_open-2.0.0.tar.gz (103 kB)\n",
            "Requirement already satisfied: six>=1.5.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
            "Requirement already satisfied: requests in /home/etherealenvy/.local/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
            "Collecting boto\n",
            "  Using cached boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
            "Requirement already satisfied: boto3 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.13.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.1)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.16.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.4->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.4->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.0.0-py3-none-any.whl size=101341 sha256=ea090b3b65b0b537ff5b71ea0f1f683dc02f3a05bb489f90eb65da4f3c619a9a\n",
            "  Stored in directory: /home/etherealenvy/.cache/pip/wheels/16/64/85/f3205b74e01a98fb81e081c0d61c2ecd04e4645a986db3726e\n",
            "Successfully built smart-open\n",
            "Installing collected packages: boto, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 gensim-3.8.3 smart-open-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB6IqAnrB0TL",
        "colab_type": "text"
      },
      "source": [
        "Gensim does not have a HTML parser like sumy. So, let us use the example text from Chapter 5 (nlphistory.txt) to see what its summarized version looks like! \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dHkUgCoB0TP",
        "colab_type": "code",
        "colab": {},
        "outputId": "5f2e675e-7be7-47a7-8bd4-3a7077aef9dc"
      },
      "source": [
        "from gensim.summarization import summarize,summarize_corpus\n",
        "from gensim.summarization.textcleaner import split_sentences\n",
        "from gensim import corpora\n",
        "\n",
        "text = open(\"nlphistory.txt\").read()\n",
        "\n",
        "#summarize method extracts the most relevant sentences in a text\n",
        "print(\"Summarize:\\n\",summarize(text, word_count=200, ratio = 0.1))\n",
        "\n",
        "\n",
        "#the summarize_corpus selects the most important documents in a corpus:\n",
        "sentences = split_sentences(text)# Creates a corpus where each document is a sentence.\n",
        "tokens = [sentence.split() for sentence in sentences]\n",
        "dictionary = corpora.Dictionary(tokens)\n",
        "corpus = [dictionary.doc2bow(sentence_tokens) for sentence_tokens in tokens]\n",
        "\n",
        "# Extracts the most important documents (shown here in BoW representation)\n",
        "print(\"-\"*30,\"\\nSummarize Corpus\\n\",summarize_corpus(corpus,ratio=0.1))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summarize:\n",
            " Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.\n",
            "This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\n",
            "However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n",
            "Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.\n",
            "In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.\n",
            "------------------------------ \n",
            "Summarize Corpus\n",
            " [[(3, 1), (7, 1), (10, 1), (11, 1), (12, 1), (22, 1), (24, 1), (27, 1), (80, 1), (94, 1), (95, 1), (193, 1), (199, 1), (214, 1), (249, 1), (262, 1), (413, 1), (418, 1), (449, 1), (450, 1), (451, 1), (452, 1), (453, 1), (454, 1), (455, 1), (456, 1), (457, 1), (458, 1), (459, 1), (460, 1), (461, 1), (462, 1), (463, 1), (464, 1)], [(11, 1), (12, 1), (13, 1), (17, 3), (38, 1), (55, 1), (57, 1), (76, 1), (82, 2), (94, 1), (164, 1), (203, 1), (206, 2), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XGqBqPQB0T7",
        "colab_type": "text"
      },
      "source": [
        "The two parameters **word_count** and **ratio** we can adjust how much text the summarizer outputs\n",
        "1. word_count: maximum amount of words we want in the summary\n",
        "2. ratio: fraction of sentences in the original text should be returned as output\n",
        "\n",
        "### Todo: Explore other options in gensim summarizer, what are possible shortcomings (e.g., sensitive to input's format etc)\n",
        "[Short-Comings\n",
        "1. gensim's summarizer uses TextRank by default, an algorithm that uses PageRank. In gensim it is unfortunately implemented using a Python list of PageRank graph nodes, so it may fail if your graph is too big.\n",
        "2.]\n",
        "\n",
        "\n",
        "\n",
        "## Summa Summarizer\n",
        "The summa summarizer uses TextRank too but with optimizations on similar functions. More information about the optimizations can be found in the following [paper](https://arxiv.org/pdf/1602.03606.pdf). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p5qBTf7B0T_",
        "colab_type": "code",
        "colab": {},
        "outputId": "76dad3b1-099d-4306-e5e3-0eae8b1ffcf3"
      },
      "source": [
        "!pip install summa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: summa in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (1.2.0)\r\n",
            "Requirement already satisfied: scipy>=0.19 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from summa) (1.4.1)\r\n",
            "Requirement already satisfied: numpy>=1.13.3 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.5/site-packages (from scipy>=0.19->summa) (1.18.4)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcBq0k1mB0UX",
        "colab_type": "code",
        "colab": {},
        "outputId": "4c8f3982-cbd9-4850-f298-9318746af1f9"
      },
      "source": [
        "from summa import summarizer\n",
        "from summa import keywords\n",
        "text = open(\"nlphistory.txt\").read()\n",
        "\n",
        "print(\"Summary:\")\n",
        "print (summarizer.summarize(text,ratio=0.1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary:\n",
            "However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n",
            "In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[4][5] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[6] parsing,[7][8] and many others.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_x7UTe3B0Up",
        "colab_type": "text"
      },
      "source": [
        "### BERT for Extractive Summarization\n",
        "Lets see how we can use BERT for extractive summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN3afk2nB0Ut",
        "colab_type": "code",
        "colab": {},
        "outputId": "232f50c2-7a72-4ff9-88c8-63626564177c"
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.10 :: Anaconda, Inc.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEGOmpwjB0VE",
        "colab_type": "code",
        "colab": {},
        "outputId": "a13ba70e-859f-467a-d2c7-48ab3db91c56"
      },
      "source": [
        "#Install the required libraries\n",
        "!pip install bert-extractive-summarizer\n",
        "!pip install spacy==2.1.3\n",
        "!pip install transformers==2.2.2\n",
        "!pip install neuralcoref\n",
        "!pip install torch #you can comment this line if u already have tensorflow2.0 installed\n",
        "!pip install neuralcoref --no-binary neuralcoref\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-extractive-summarizer in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (0.4.2)\n",
            "Requirement already satisfied: transformers in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from bert-extractive-summarizer) (2.2.2)\n",
            "Requirement already satisfied: spacy in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from bert-extractive-summarizer) (2.1.3)\n",
            "Requirement already satisfied: scikit-learn in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from bert-extractive-summarizer) (0.22.2.post1)\n",
            "Requirement already satisfied: boto3 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from transformers->bert-extractive-summarizer) (1.13.4)\n",
            "Requirement already satisfied: regex in /home/etherealenvy/.local/lib/python3.6/site-packages (from transformers->bert-extractive-summarizer) (2020.4.4)\n",
            "Requirement already satisfied: numpy in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from transformers->bert-extractive-summarizer) (1.18.4)\n",
            "Requirement already satisfied: sacremoses in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from transformers->bert-extractive-summarizer) (0.0.43)\n",
            "Requirement already satisfied: tqdm in /home/etherealenvy/.local/lib/python3.6/site-packages (from transformers->bert-extractive-summarizer) (4.46.0)\n",
            "Requirement already satisfied: requests in /home/etherealenvy/.local/lib/python3.6/site-packages (from transformers->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from transformers->bert-extractive-summarizer) (0.1.86)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy->bert-extractive-summarizer) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy->bert-extractive-summarizer) (0.6.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy->bert-extractive-summarizer) (7.0.8)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy->bert-extractive-summarizer) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy->bert-extractive-summarizer) (2.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy->bert-extractive-summarizer) (1.0.2)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy->bert-extractive-summarizer) (2.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy->bert-extractive-summarizer) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy->bert-extractive-summarizer) (2.0.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/etherealenvy/.local/lib/python3.6/site-packages (from scikit-learn->bert-extractive-summarizer) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->transformers->bert-extractive-summarizer) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->transformers->bert-extractive-summarizer) (1.16.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->transformers->bert-extractive-summarizer) (0.9.5)\n",
            "Requirement already satisfied: six in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.14.0)\n",
            "Requirement already satisfied: click in /home/etherealenvy/.local/lib/python3.6/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->transformers->bert-extractive-summarizer) (1.25.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->transformers->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->transformers->bert-extractive-summarizer) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->transformers->bert-extractive-summarizer) (2.9)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers->bert-extractive-summarizer) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers->bert-extractive-summarizer) (0.15.2)\n",
            "Requirement already satisfied: spacy==2.1.3 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (2.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy==2.1.3) (1.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy==2.1.3) (0.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/etherealenvy/.local/lib/python3.6/site-packages (from spacy==2.1.3) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy==2.1.3) (1.18.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy==2.1.3) (7.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy==2.1.3) (0.6.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy==2.1.3) (0.9.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy==2.1.3) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy==2.1.3) (2.0.3)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy==2.1.3) (2.6.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy==2.1.3) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (1.25.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (2.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/etherealenvy/.local/lib/python3.6/site-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.3) (4.46.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.2.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /home/etherealenvy/.local/lib/python3.6/site-packages (from transformers==2.2.2) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from transformers==2.2.2) (0.1.86)\n",
            "Requirement already satisfied: sacremoses in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from transformers==2.2.2) (0.0.43)\n",
            "Requirement already satisfied: regex in /home/etherealenvy/.local/lib/python3.6/site-packages (from transformers==2.2.2) (2020.4.4)\n",
            "Requirement already satisfied: numpy in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from transformers==2.2.2) (1.18.4)\n",
            "Requirement already satisfied: boto3 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from transformers==2.2.2) (1.13.4)\n",
            "Requirement already satisfied: tqdm in /home/etherealenvy/.local/lib/python3.6/site-packages (from transformers==2.2.2) (4.46.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->transformers==2.2.2) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->transformers==2.2.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->transformers==2.2.2) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests->transformers==2.2.2) (1.25.9)\n",
            "Requirement already satisfied: click in /home/etherealenvy/.local/lib/python3.6/site-packages (from sacremoses->transformers==2.2.2) (7.1.2)\n",
            "Requirement already satisfied: six in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from sacremoses->transformers==2.2.2) (1.14.0)\n",
            "Requirement already satisfied: joblib in /home/etherealenvy/.local/lib/python3.6/site-packages (from sacremoses->transformers==2.2.2) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->transformers==2.2.2) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->transformers==2.2.2) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->transformers==2.2.2) (1.16.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers==2.2.2) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: neuralcoref in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/etherealenvy/.local/lib/python3.6/site-packages (from neuralcoref) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from neuralcoref) (1.18.4)\n",
            "Requirement already satisfied: boto3 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from neuralcoref) (1.13.4)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from neuralcoref) (2.1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.25.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->neuralcoref) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->neuralcoref) (1.16.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->neuralcoref) (0.3.3)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (0.6.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (2.0.3)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.4->boto3->neuralcoref) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.4->boto3->neuralcoref) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/etherealenvy/.local/lib/python3.6/site-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.46.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.4->boto3->neuralcoref) (1.14.0)\n",
            "Requirement already satisfied: torch in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (1.5.0)\n",
            "Requirement already satisfied: numpy in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from torch) (1.18.4)\n",
            "Requirement already satisfied: future in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from torch) (0.18.2)\n",
            "Requirement already satisfied: neuralcoref in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (4.0)\n",
            "Requirement already satisfied: boto3 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from neuralcoref) (1.13.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from neuralcoref) (1.18.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/etherealenvy/.local/lib/python3.6/site-packages (from neuralcoref) (2.23.0)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from neuralcoref) (2.1.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->neuralcoref) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->neuralcoref) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from boto3->neuralcoref) (1.16.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.25.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.4->boto3->neuralcoref) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.4->boto3->neuralcoref) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/etherealenvy/.local/lib/python3.6/site-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.46.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.4->boto3->neuralcoref) (1.14.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAmOJjZvB0VX",
        "colab_type": "code",
        "colab": {},
        "outputId": "e875e731-e434-49e6-f49c-87d0ca0c699a"
      },
      "source": [
        "#sowyma could you please look at this coreference vs without coreference. I personally think we need to use a better input.\n",
        "#currently using the same one as above the nlphistory.txt\n",
        "\n",
        "from summarizer import Summarizer\n",
        "from summarizer.coreference_handler import CoreferenceHandler\n",
        "\n",
        "model = Summarizer()\n",
        "\n",
        "print(\"Without Coreference:\")\n",
        "result = model(text, min_length=200,ratio=0.01)\n",
        "full = ''.join(result)\n",
        "print(full)\n",
        "\n",
        "\n",
        "print(\"With Coreference:\")\n",
        "handler = CoreferenceHandler(greedyness=.35)\n",
        "\n",
        "model = Summarizer(sentence_handler=handler)\n",
        "result = model(text, min_length=200,ratio=0.01)\n",
        "full = ''.join(result)\n",
        "print(full)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without Coreference:\n",
            "The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\n",
            "With Coreference:\n",
            "However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OwLSKz0B0Vx",
        "colab_type": "text"
      },
      "source": [
        "We are done with discussing different Extractive Summarization techniques and examples. Lets move on to Abstractive Summarization.\n",
        "## Abstractive Summariazation\n",
        "There have been even efforts to use **RL** for summarization.<br>\n",
        "The past few years **RNN**s using encoder — decoder models have become popular for abstractive summarization. <br>\n",
        "Recently **Transformers** which use attention mechanism have become popular for abstractive summarization. \n",
        "\n",
        "As mentioned in Ch7  abstractive summarization is more of a research topic than a practical application. \n",
        "\n",
        "We will demo simple abstractive text summarization with pretrained T5 — Text-To-Text Transfer Transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94W3znKkB0V0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "d3475800-6114-494d-fb8f-c526ce15ab6b"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\r\u001b[K     |▌                               | 10kB 27.3MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 8.8MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 10.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 9.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 10.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 112kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 225kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 307kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 368kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 389kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 419kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 450kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 481kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 501kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 532kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 563kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 583kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 593kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 614kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 9.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 58.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=db5b3ec704552fa293df9d1120595ec82159761c8bf1a5bb0aa39b06b3376aed\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.7.0 transformers-2.9.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7sj1wDtGz5r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "23365e1f-7031-4793-a166-1fd64ab45884"
      },
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "#the maximum length of tokens is lmited to 512\n",
        "text =\"\"\"\n",
        "The US has \"passed the peak\" on new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month.\n",
        "The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.\n",
        "At the daily White House coronavirus briefing on Wednesday, Trump said new guidelines to reopen the country would be announced on Thursday after he speaks to governors.\n",
        "\"We'll be the comeback kids, all of us,\" he said. \"We want to get our country back.\"\n",
        "The Trump administration has previously fixed May 1 as a possible date to reopen the world's largest economy, but the president said some states may be able to return to normalcy earlier than that.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
        "t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "print (\"original text preprocessed: \\n\", preprocess_text)\n",
        "\n",
        "tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "# summmarize \n",
        "summary_ids = model.generate(tokenized_text,\n",
        "                                    num_beams=4,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    min_length=30,\n",
        "                                    max_length=100,\n",
        "                                    early_stopping=True)\n",
        "#there are more parameters which can be found at https://huggingface.co/transformers/model_doc/t5.html\n",
        "\n",
        "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print (\"\\n\\nSummarized text: \\n\",output)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original text preprocessed: \n",
            " The US has \"passed the peak\" on new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month.The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.At the daily White House coronavirus briefing on Wednesday, Trump said new guidelines to reopen the country would be announced on Thursday after he speaks to governors.\"We'll be the comeback kids, all of us,\" he said. \"We want to get our country back.\"The Trump administration has previously fixed May 1 as a possible date to reopen the world's largest economy, but the president said some states may be able to return to normalcy earlier than that.\n",
            "\n",
            "\n",
            "Summarized text: \n",
            " the us has over 637,000 confirmed Covid-19 cases and over 30,826 deaths. president Donald Trump predicts some states will reopen the country in april, he said. \"we'll be the comeback kids, all of us,\" the president says.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvpff-KJH84c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}